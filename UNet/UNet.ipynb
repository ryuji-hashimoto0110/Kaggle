{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "[こちらのサイト](https://github.com/yoyoyo-yo/DeepLearningMugenKnock/blob/master/notes_pytorch/ImgSeg/UNet_VOC2012_pytorch.ipynb)を参考にUNetを実装する。UNetは全結合層を持たず、畳み込み層のみで構成されている。左右対称のEncoder-Decoder構造で、Encoderのpoolingを経てダウンサンプリングされた特徴マップをDecoderでアップサンプリングしていく。SegNetとの大きな違いは、Encoderの各層で出力される特徴マップをDecoderの対応する各層の特徴マップに連結(concatenation)するアプローチを導入した点。このアプローチはスキップ接続と呼ばれている。\n",
    "\n",
    "セグメンテーションではピクセル単位で推論する。ラベルの形状は(-1,128,128)で、各ピクセルは22通りの値となっているので、モデルの出力は(-1,22,128,128)\n",
    "\n",
    "<img src = 'module.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as Ap\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_HEIGHT, IN_WIDTH = 128, 128\n",
    "\n",
    "FOLD = 'KFOLD'\n",
    "FOLD_N = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim1, dim2, stack_num = 2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        module = []\n",
    "        for i in range(stack_num):\n",
    "            f = dim1 if i == 0 else dim2\n",
    "            module.append(nn.Conv2d(f, dim2, kernel_size = 3,\n",
    "                                    padding = 1, stride = 1))\n",
    "            module.append(nn.BatchNorm2d(dim2))\n",
    "            module.append(nn.ReLU())\n",
    "            \n",
    "        self.module = nn.Sequential(*module)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input: (-1, dim1, H_in, W_in)\n",
    "        # output: (-1, dim2, H_in, W_in)\n",
    "        return self.module(x)\n",
    "    \n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.ConvTranspose2d(dim1, dim2, kernel_size = 2, stride = 2),\n",
    "            nn.BatchNorm2d(dim2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input: (-1, dim1, H_in, W_in)\n",
    "        # output: (-1, dim2, H_in*2, W_in*2)\n",
    "        return self.module(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, dim = 64, in_channel = 3, out_channel = 22):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.encoder1 = Encoder(in_channel, dim)\n",
    "        self.encoder2 = Encoder(dim, dim * 2)\n",
    "        self.encoder3 = Encoder(dim * 2, dim * 4)\n",
    "        self.encoder4 = Encoder(dim * 4, dim * 8)\n",
    "        self.encoder5 = Encoder(dim * 8, dim * 16)\n",
    "        \n",
    "        self.upconv4 = UpConv(dim * 16, dim * 8)\n",
    "        self.decoder4 = Encoder(dim * 16, dim * 8)\n",
    "        \n",
    "        self.upconv3 = UpConv(dim * 8, dim * 4)\n",
    "        self.decoder3 = Encoder(dim * 8, dim * 4)\n",
    "        \n",
    "        self.upconv2 = UpConv(dim * 4, dim * 2)\n",
    "        self.decoder2 = Encoder(dim * 4, dim * 2)\n",
    "        \n",
    "        self.upconv1 = UpConv(dim * 2, dim)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            Encoder(dim * 2, dim),\n",
    "            nn.Conv2d(dim, out_channel, kernel_size = 1,\n",
    "                      padding = 0, stride = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_e1 = self.encoder1(x)\n",
    "        x = F.max_pool2d(x_e1, 2, stride = 2, padding = 0)\n",
    "        x_e2 = self.encoder2(x)\n",
    "        x = F.max_pool2d(x_e2, 2, stride = 2, padding = 0)\n",
    "        x_e3 = self.encoder3(x)\n",
    "        x = F.max_pool2d(x_e3, 2, stride = 2, padding = 0)\n",
    "        x_e4 = self.encoder4(x)\n",
    "        x = F.max_pool2d(x_e4, 2, stride = 2, padding = 0)\n",
    "        x = self.encoder5(x)\n",
    "        \n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat([x, x_e4], dim = 1)\n",
    "        x = self.decoder4(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, x_e3], dim = 1)\n",
    "        x = self.decoder3(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, x_e2], dim = 1)\n",
    "        x = self.decoder2(x)\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, x_e1], dim = 1)\n",
    "        x = self.decoder1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.Resize(IN_HEIGHT, IN_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Normalize(max_pixel_value=255.0, p=1.0),\n",
    "    Ap.ToTensorV2(p=1.0),\n",
    "])\n",
    "\n",
    "transforms_val = A.Compose([\n",
    "    A.Resize(IN_HEIGHT, IN_WIDTH),\n",
    "    A.Normalize(max_pixel_value=255.0, p=1.0),\n",
    "    Ap.ToTensorV2(p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, Xs, ts, transforms = None, ignore_border = True):\n",
    "        self.Xs = Xs\n",
    "        self.ts = ts\n",
    "        self.transforms = transforms\n",
    "        self.data_num = len(Xs)\n",
    "        self.ignore_border = ignore_border\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.Xs[idx]\n",
    "        t = self.ts[idx]\n",
    "        \n",
    "        if self.ignore_border:\n",
    "            t[t ==  255] = 0\n",
    "            \n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image = x, mask = t)\n",
    "            x = transformed['image']\n",
    "            t = transformed['mask']\n",
    "            \n",
    "        return x, t.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(Xs, show_num = 8, name = 'input'):\n",
    "    # (8, 3, H, W)\n",
    "    \n",
    "    Xs = Xs.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    # (8, H, W, 3)\n",
    "    Xs -= Xs.min()\n",
    "    Xs /= Xs.max()\n",
    "    Xs = (Xs * 255).astype(np.uint8)\n",
    "    \n",
    "    plt.figure(figsize = (12,1))\n",
    "    \n",
    "    for i in range(show_num):\n",
    "        x = Xs[i]\n",
    "        plt.subplot(1, show_num, i+1)\n",
    "        plt.imshow(x)\n",
    "        plt.title(name)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def show_sample_seg(Xs, show_num = 8, name = 'output'):\n",
    "    # (8, 22, H, W)\n",
    "    \n",
    "    Xs = Xs.detach().cpu().numpy().argmax(axis = 1)\n",
    "    # (8, H, W)\n",
    "    \n",
    "    for i in range(show_num):\n",
    "        x = Xs[i]\n",
    "        plt.subplot(1, show_num, i+1)\n",
    "        plt.imshow(x, cmap = 'jet', vmax = 21)\n",
    "        # cmap = 'jet'で　最大値は21\n",
    "        # https://beiznotes.org/matplot-cmap-list/\n",
    "        \n",
    "        plt.title(name)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def show_sample_label(Xs, show_num = 8, name = 'label'):\n",
    "    # (8, H, W)\n",
    "    \n",
    "    Xs = Xs.detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize = (12, 1))\n",
    "    \n",
    "    for i in range(show_num):\n",
    "        x = Xs[i]\n",
    "        plt.subplot(1, show_num, i+1)\n",
    "        plt.title(x, cmap = 'jet', vmax = 21)\n",
    "        plt.title(name)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    ds = torchvision.datasets.VOCSegmentation(root = './', \n",
    "                                              image_set = 'train', download = True)\n",
    "    _inds = np.arange(len(ds))\n",
    "    \n",
    "    # fold\n",
    "    if FOLD == 'KFOLD':\n",
    "        kf = KFold(n_splits = FOLD_N, shuffle = True, random_state = RANDOM_SEED)\n",
    "        spl = kf.split(_inds)\n",
    "    \n",
    "    elif FOLD == 'GroupKFold':\n",
    "        kf = GroupKFold(n_splits = FOLD_N )\n",
    "        spl = kf.split(_inds)\n",
    "        \n",
    "    elif FOLD == 'StatifiedKFold':\n",
    "        kf = StratifiedKFold(n_splits = FOLD_N, shuffle = True, random_state = RANDOM_SEED)\n",
    "        spl = kf.split(_inds, _inds)\n",
    "        \n",
    "    else:\n",
    "        print('invalid fold')\n",
    "        return None\n",
    "    \n",
    "    train_models = []\n",
    "    train_model_paths = []\n",
    "    \n",
    "    EPOCH = 200\n",
    "    \n",
    "    for fold_i, (train_idx, val_idx) in enumerate(spl):\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        print(f'{FOLD} fold:{fold_i + 1}/{FOLD_N}')\n",
    "        print(f'train_N={len(train_idx)}, val_N={len(val_idx)}')\n",
    "        \n",
    "        #---\n",
    "        # dataset\n",
    "        #---\n",
    "        \n",
    "        X_train = [np.array(ds[idx][0], dtype = np.float32) for idx  in train_idx]\n",
    "        # X_train = ds.data[train_idx]で良さそう\n",
    "        X_val = [np.array(ds[idx][0], dtype = np.float32) for idx in val_idx]\n",
    "        t_train = [np.array(ds[idx][1].convert('P')) for idx in train_idx]\n",
    "        t_val = [np.array(ds[idx][1].convert('P')) for idx in val_idx]\n",
    "        \n",
    "        dataset_train = VOCDataset(X_train, t_train, transforms = transforms_train)\n",
    "        dataset_val = VOCDataset(X_val, t_val, transforms = transforms_val)\n",
    "        \n",
    "        dataloader_train = DataLoader(dataset_train, batch_size = 16, \n",
    "                                      num_workers = 0, shuffle = True, pin_memory = False)\n",
    "        dataloader_val = DataLoader(dataset_val, batch_size = 16,\n",
    "                                    num_workers = 0, shuffle = False, pin_memory = False)\n",
    "        \n",
    "        train_n = len(X_train)\n",
    "        val_n = len(X_val)\n",
    "        \n",
    "        #---\n",
    "        # model\n",
    "        #---\n",
    "        \n",
    "        model = UNet()\n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "        \n",
    "        \n",
    "        #---\n",
    "        # epoch\n",
    "        #---\n",
    "        \n",
    "        for epoch in range(EPOCH):\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            tr_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            #---\n",
    "            # train\n",
    "            #---\n",
    "            \n",
    "            train_time_start = time.time()\n",
    "            \n",
    "            for step, batch in enumerate(dataloader_train):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                xs = batch[0].to(device)\n",
    "                ts = batch[1].to(device)\n",
    "                # (-1, 128, 128)\n",
    "                \n",
    "                ys = model(xs)\n",
    "                # (-1, 22, 128, 128)\n",
    "                \n",
    "                _ys = ys.permute(0, 2, 3, 1).reshape(-1, 22)\n",
    "                # (-1 * 128 * 128, 22)\n",
    "                # 128*128がデータ数分\n",
    "                _ts = ts.view(-1)\n",
    "                # (-1 * 128 * 128)\n",
    "                \n",
    "                loss = criterion(_ys, _ts) / train_n / IN_HEIGHT / IN_WIDTH\n",
    "                loss.backward()\n",
    "                \n",
    "                loss = loss.item()\n",
    "                tr_loss += loss\n",
    "                \n",
    "                _, predicted = torch.max(ys.data, 1)\n",
    "                total += ys.size(0)\n",
    "                correct += (predicted == ts).sum().item()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "            train_losses.append(tr_loss)\n",
    "            \n",
    "            train_accuracy = correct / total / IN_HEIGHT / IN_WIDTH\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            train_time_end = time.time()\n",
    "            \n",
    "            #---\n",
    "            # val\n",
    "            #---\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            val_time_start = time.time()\n",
    "            \n",
    "            val_labels = []\n",
    "            val_preds = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for step, batch in enumerate(dataloader_val):\n",
    "                    xs = batch[0].to(device)\n",
    "                    ts = batch[1].to(device)\n",
    "                    \n",
    "                    ys = model(xs)\n",
    "                    \n",
    "                    _ys = ys.permute(0, 2, 3, 1).reshape(-1, 22)\n",
    "                    _ts = ts.view(-1)\n",
    "                    \n",
    "                    loss = criterion(_ys, _ts)\n",
    "                    val_loss += loss.item() / val_n / IN_HEIGHT / IN_WIDTH\n",
    "                    \n",
    "                    _, predicted = torch.max(ys.data, 1)\n",
    "                    val_total += ys.size(0)\n",
    "                    val_correct += (predicted == ts).sum().item()\n",
    "                    \n",
    "            val_time_end = time.time()\n",
    "            train_time_total = train_time_end - train_time_start\n",
    "            val_time_total = val_time_end - val_time_start\n",
    "            total_time = train_time_total + val_time_total\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            val_accuracy = val_correct / val_total / IN_HEIGHT / IN_WIDTH\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f'fold:{fold_i + 1} epoch:{epoch + 1}/{EPOCH} [tra]loss: {tr_loss:.4f} acc: {train_accuracy:.4f} [val]loss: {val_loss:.4f} acc:{val_accuracy:.4f} [time]total: {total_time:.2f}sec tra:{train_time_total:.2f}sec val:{val_time_total:.2f}sec')\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                show_sample(xs)\n",
    "                show_sample_seg(ys)\n",
    "                show_sample_label(ts)\n",
    "                \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                savename = f'model_epoch{epoch + 1}_{EPOCH}_{FOLD}_{fold_i + 1}_{FOLD_N}.pth'\n",
    "                torch.save(model.state_dict(), savename)\n",
    "                print(f'model saved to >> {savename}')\n",
    "        \n",
    "        #---\n",
    "        # save model\n",
    "        #---\n",
    "        \n",
    "        savename = f'model_epoch{EPOCH}_{FOLD}_{fold_i}_{FOLD_N}.pth'\n",
    "        torch.save(model.state_dict(), savename)\n",
    "        print(f'model saved to >> {savename}')\n",
    "        \n",
    "        train_models.append(model)\n",
    "        train_model_paths.append(savename)\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax2 = ax1.twinx()\n",
    "        ax1.grid()\n",
    "        ax1.plot(train_losses, marker = '.', markersize = 6, color = 'red', label = 'train loss')\n",
    "        ax1.plot(val_losses, marker = '.', markersize = 6, color = 'blue', label = 'val losses')\n",
    "        ax2.plot(train_accuracies, marker = '.', markersize = 6, color = 'green', label = 'train accuracy')\n",
    "        h1, l1 = ax1.get_legend_handles_labels()\n",
    "        h2, l2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(h1 + h2, l1 + l2, loc = 'upper right')\n",
    "        ax1.set(xlabel = 'Epoch', ylabel = 'Loss')\n",
    "        ax2.set(ylabel = 'Accuracy')\n",
    "        \n",
    "        break\n",
    "        \n",
    "    return train_models, train_model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./VOCtrainval_11-May-2012.tar\n",
      "KFOLD fold:1/2\n",
      "train_N=732, val_N=732\n",
      "fold:1 epoch:1/200 [tra]loss: 2.9165 acc: 0.3433 [val]loss: 2.8415 acc:0.4367 [time]total: 1645.97sec tra:1296.47sec val:349.50sec\n",
      "fold:1 epoch:2/200 [tra]loss: 2.5667 acc: 0.5608 [val]loss: 2.6816 acc:0.6049 [time]total: 1671.01sec tra:1284.71sec val:386.30sec\n",
      "fold:1 epoch:3/200 [tra]loss: 2.4096 acc: 0.6569 [val]loss: 2.4354 acc:0.6803 [time]total: 1973.96sec tra:1641.68sec val:332.28sec\n",
      "fold:1 epoch:4/200 [tra]loss: 2.2817 acc: 0.6759 [val]loss: 2.1872 acc:0.6199 [time]total: 1490.37sec tra:1161.82sec val:328.56sec\n",
      "fold:1 epoch:5/200 [tra]loss: 2.1460 acc: 0.6184 [val]loss: 2.0165 acc:0.6878 [time]total: 1504.64sec tra:1177.33sec val:327.31sec\n",
      "fold:1 epoch:6/200 [tra]loss: 2.0322 acc: 0.6303 [val]loss: 2.0368 acc:0.6262 [time]total: 1497.87sec tra:1170.84sec val:327.03sec\n",
      "fold:1 epoch:7/200 [tra]loss: 1.9252 acc: 0.6813 [val]loss: 1.7931 acc:0.7142 [time]total: 1507.65sec tra:1179.57sec val:328.08sec\n",
      "fold:1 epoch:8/200 [tra]loss: 1.8244 acc: 0.7100 [val]loss: 1.8822 acc:0.7078 [time]total: 1496.41sec tra:1169.49sec val:326.92sec\n",
      "fold:1 epoch:9/200 [tra]loss: 1.7349 acc: 0.7300 [val]loss: 1.8897 acc:0.7209 [time]total: 1493.60sec tra:1165.89sec val:327.71sec\n",
      "fold:1 epoch:10/200 [tra]loss: 1.6595 acc: 0.7427 [val]loss: 1.6541 acc:0.7523 [time]total: 1498.87sec tra:1171.01sec val:327.85sec\n",
      "fold:1 epoch:11/200 [tra]loss: 1.5807 acc: 0.7465 [val]loss: 1.5385 acc:0.7527 [time]total: 1498.65sec tra:1172.43sec val:326.22sec\n",
      "fold:1 epoch:12/200 [tra]loss: 1.5111 acc: 0.7446 [val]loss: 1.4744 acc:0.7531 [time]total: 1498.59sec tra:1172.38sec val:326.21sec\n",
      "fold:1 epoch:13/200 [tra]loss: 1.4569 acc: 0.7456 [val]loss: 1.4346 acc:0.7540 [time]total: 1490.54sec tra:1164.02sec val:326.51sec\n",
      "fold:1 epoch:14/200 [tra]loss: 1.4031 acc: 0.7467 [val]loss: 1.3488 acc:0.7536 [time]total: 1502.36sec tra:1175.84sec val:326.52sec\n",
      "fold:1 epoch:15/200 [tra]loss: 1.3605 acc: 0.7478 [val]loss: 1.3562 acc:0.7533 [time]total: 1494.35sec tra:1167.62sec val:326.72sec\n",
      "fold:1 epoch:16/200 [tra]loss: 1.3173 acc: 0.7467 [val]loss: 1.5096 acc:0.7253 [time]total: 1496.31sec tra:1169.41sec val:326.90sec\n",
      "fold:1 epoch:17/200 [tra]loss: 1.2855 acc: 0.7457 [val]loss: 1.2764 acc:0.7502 [time]total: 1503.60sec tra:1173.10sec val:330.50sec\n",
      "fold:1 epoch:18/200 [tra]loss: 1.2386 acc: 0.7474 [val]loss: 1.2093 acc:0.7509 [time]total: 1498.73sec tra:1170.50sec val:328.23sec\n",
      "fold:1 epoch:19/200 [tra]loss: 1.2065 acc: 0.7493 [val]loss: 1.2152 acc:0.7507 [time]total: 1505.08sec tra:1175.76sec val:329.32sec\n",
      "fold:1 epoch:20/200 [tra]loss: 1.1803 acc: 0.7473 [val]loss: 1.1845 acc:0.7530 [time]total: 1500.68sec tra:1173.57sec val:327.11sec\n",
      "fold:1 epoch:21/200 [tra]loss: 1.1563 acc: 0.7485 [val]loss: 1.2644 acc:0.7078 [time]total: 1499.29sec tra:1172.29sec val:327.00sec\n",
      "fold:1 epoch:22/200 [tra]loss: 1.1288 acc: 0.7513 [val]loss: 1.1507 acc:0.7466 [time]total: 1496.03sec tra:1168.27sec val:327.77sec\n",
      "fold:1 epoch:23/200 [tra]loss: 1.1117 acc: 0.7496 [val]loss: 1.1477 acc:0.7488 [time]total: 1475.88sec tra:1148.49sec val:327.39sec\n",
      "fold:1 epoch:24/200 [tra]loss: 1.0900 acc: 0.7529 [val]loss: 1.1341 acc:0.7439 [time]total: 1493.96sec tra:1165.84sec val:328.13sec\n",
      "fold:1 epoch:25/200 [tra]loss: 1.0861 acc: 0.7503 [val]loss: 1.1014 acc:0.7516 [time]total: 1545.33sec tra:1166.16sec val:379.17sec\n",
      "fold:1 epoch:26/200 [tra]loss: 1.0692 acc: 0.7534 [val]loss: 1.0657 acc:0.7570 [time]total: 1417.48sec tra:1101.78sec val:315.70sec\n",
      "fold:1 epoch:27/200 [tra]loss: 1.0613 acc: 0.7522 [val]loss: 1.0729 acc:0.7509 [time]total: 1582.24sec tra:1218.16sec val:364.07sec\n",
      "fold:1 epoch:28/200 [tra]loss: 1.0388 acc: 0.7547 [val]loss: 1.0889 acc:0.7508 [time]total: 1563.24sec tra:1177.83sec val:385.41sec\n"
     ]
    }
   ],
   "source": [
    "train_models, train_model_paths = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
